ModuleList(
  (0): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (dropout_module): FairseqDropout()
    (activation_dropout_module): FairseqDropout()
    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
    (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (drop_path): Identity()
  )
  (1): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (dropout_module): FairseqDropout()
    (activation_dropout_module): FairseqDropout()
    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
    (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (drop_path): Identity()
  )
  (2): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (dropout_module): FairseqDropout()
    (activation_dropout_module): FairseqDropout()
    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
    (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (drop_path): Identity()
  )
  (3): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (dropout_module): FairseqDropout()
    (activation_dropout_module): FairseqDropout()
    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
    (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (drop_path): Identity()
  )
  (4): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (dropout_module): FairseqDropout()
    (activation_dropout_module): FairseqDropout()
    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
    (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (drop_path): Identity()
  )
  (5): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (dropout_module): FairseqDropout()
    (activation_dropout_module): FairseqDropout()
    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
    (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (drop_path): Identity()
  )
  (6): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (dropout_module): FairseqDropout()
    (activation_dropout_module): FairseqDropout()
    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
    (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (drop_path): Identity()
  )
  (7): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (dropout_module): FairseqDropout()
    (activation_dropout_module): FairseqDropout()
    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
    (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (drop_path): Identity()
  )
  (8): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (dropout_module): FairseqDropout()
    (activation_dropout_module): FairseqDropout()
    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
    (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (drop_path): Identity()
  )
  (9): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (dropout_module): FairseqDropout()
    (activation_dropout_module): FairseqDropout()
    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
    (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (drop_path): Identity()
  )
  (10): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (dropout_module): FairseqDropout()
    (activation_dropout_module): FairseqDropout()
    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
    (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (drop_path): Identity()
  )
  (11): TransformerEncoderLayer(
    (self_attn): MultiheadAttention(
      (dropout_module): FairseqDropout()
      (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
      (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (dropout_module): FairseqDropout()
    (activation_dropout_module): FairseqDropout()
    (fc1): Linear(in_features=1024, out_features=4096, bias=True)
    (fc2): Linear(in_features=4096, out_features=1024, bias=True)
    (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (ffn_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (drop_path): Identity()
  )
)
